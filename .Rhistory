final_model_ch<-Arima(ts_China, order = c(0,2,0))
checkresiduals(final_model_ch)
pred_ch<-forecast(final_model_ch, h=2)
pred_ch
final_model_in<-ets(ts_India)
summary(final_model_in)
checkresiduals(final_model_in)
pred_in<-forecast(final_model_in, h=2)
pred_in
?ets
summary(smooth_model_train_in)
final_model_in<-ets(ts_India, model="MAN")
summary(final_model_in)
checkresiduals(final_model_in)
pred_in<-forecast(final_model_in, h=2)
pred_in
final_model_in<-ets(ts_India)
summary(final_model_in)
checkresiduals(final_model_in)
pred_in<-forecast(final_model_in, h=2)
pred_in
final_model_in<-ets(ts_India, method="MAN")
final_model_in<-ets(ts_India, model="MAN")
summary(final_model_in)
checkresiduals(final_model_in)
pred_in<-forecast(final_model_in, h=2)
pred_in
summary(auto.arima(training_in))
autoplot(ts_Singapore)
ts_data<-spread(transformed_data,Country,LaborRate)
pacman::p_load(forecast, tseries, fUnitRoots, tidyverse, lmtest, fastDummies)
ts_China<-ts(ts_data$China, start = 1980, frequency = 1)
ts_India<-ts(ts_data$India, start = 1980, frequency = 1)
ts_Singapore<-ts(ts_data$Singapore,start = 1980, frequency = 1)
autoplot(ts_Singapore)
training_sg<-subset(ts_Singapore, end=26)
val_sg<-subset(ts_Singapore, start=27)
adfTest(training_sg)
summary(auto.arima(training_sg))
training_sg %>% diff() %>% ggtsdisplay()
training_sg %>% diff() %>% ggtsdisplay()
training_sg %>% diff() %>% adfTest()
model_train_sg<-Arima(training_sg, order=c(1,1,1))
summary(model_train_sg)
coeftest(model_train_sg)
summary(auto.arima(training_sg))
summary(auto.arima(training_in))
summary(auto.arima(training_ch))
summary(auto.arima(ts_Singapore))
training_sg %>% diff() %>% ggtsdisplay()
training_sg %>% diff() %>% adfTest()
training_sg %>% diff() %>% diff() %>% ggtsdisplay()
training_sg %>% diff() %>% diff() %>% adfTest()
model_train_sg<-Arima(training_sg, order=c(1,2,1))
summary(model_train_sg)
coeftest(model_train_sg)
autoplot(ts_Singapore)
training_sg<-subset(ts_Singapore, end=26)
val_sg<-subset(ts_Singapore, start=27)
adfTest(training_sg)
summary(auto.arima(training_sg))
coeftest(auto.arima(training_sg))
model_train_sg<-Arima(training_sg, order=c(1,1,1))
summary(model_train_sg)
coeftest(model_train_sg)
model_train_sg<-Arima(training_sg, order=c(1,1,0))
summary(model_train_sg)
coeftest(model_train_sg)
?Arima
model_train_sg<-Arima(training_sg, order=c(1,2,1))
summary(model_train_sg)
coeftest(model_train_sg)
model_train_sg<-Arima(training_sg, order=c(1,2,1))
summary(model_train_sg)
coeftest(model_train_sg)
arima_model_train_sg<-model_train_sg
checkresiduals(arima_model_train_sg)
smooth_model_train_sg<-ets(training_sg)
summary(smooth_model_train_sg)
?ets
smooth_model_train_sg<-ets(training_sg)
summary(smooth_model_train_sg)
checkresiduals(smooth_model_train_sg)
pred_arima_sg<-forecast(arima_model_train_sg, h=2)
pred_smooth_sg<-forecast(smooth_model_train_sg, h=2)
accuracy(pred_arima_sg, val_sg)
accuracy(pred_smooth_sg, val_sg)
final_model_sg<-Arima(ts_Singapore, order = c(1,2,1))
summary(final_model_sg)
pred_sg<-forecast(final_model_sg, h=2)
pred_sg
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/3.PA/CA/data")
pacman::p_load(tidyverse, readxl, lubridate, mice, caret, rpart, randomForest, fastAdaboost, ROCR, caretEnsemble)
data<-read.csv("reports.csv", stringsAsFactors = FALSE)
dim(data)
summary(data)
table(data$Insp=="unkn")
table(data$Insp)
rate<-1270/15732*100
rate
md.pattern(data)
md.pattern(data[data$Insp!="unkn",])
md.pattern(data[data$Insp=="fraud",])
ggplot(data[data$Insp!="unkn",], aes(Quant, Val, color=Insp))+
geom_point(alpha=0.5, na.rm = TRUE, size=0.5)+
scale_x_continuous(trans= "log") + scale_y_continuous(trans= "log")
data$Missing<-"None Missng"
data[is.na(data$Quant),]$Missing<-"Quant Missing"
data[is.na(data$Val),]$Missing<-"Val Missing"
data[is.na(data$Quant)&is.na(data$Val),]$Missing<-"Both Missing"
table(data$Missing)
data <- data %>% filter(Missing!="Both Missing")
table(data$Missing)
data$unit_price<-data$Val/data$Quant
price_data<-data %>% filter(Insp!="fraud") %>% group_by(Prod) %>%
summarise(med_price=median(unit_price, na.rm = TRUE),iqr=IQR(unit_price, na.rm = TRUE))
table(is.na(price_data$med_price))
price_data<-price_data %>% filter(!is.na(med_price))
data <- data %>% inner_join(price_data, by="Prod")
noQuant<-data$Missing=="Quant Missing"
data[noQuant, "Quant"]=ceiling(data[noQuant, "Val"]/data[noQuant, "med_price"])
noVal<-data$Missing=="Val Missing"
data[noVal, "Val"]=data[noVal, "Quant"]*data[noVal, "med_price"]
data$unit_price<-data$Val/data$Quant
md.pattern(data)
data$relative_price<-2*(data$unit_price-data$med_price)/(data$unit_price+data$med_price)
data$Missing<-as.factor(data$Missing)
inspected<-data[data$Insp!="unkn",]
test<-data[data$Insp=="unkn",]
set.seed(1234)
inTrain<-createDataPartition(inspected$Insp, p=0.7, list = FALSE)
training<-inspected[inTrain,]
validation<-inspected[-inTrain,]
## build decision tree model
DTmodel<-rpart(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
## build decision tree model
DTmodel<-rpart(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
DTpred<-predict(DTmodel, validation, type="class")
DTpred<-predict(DTmodel, validation, type="class")
confusionMatrix(table(validation$Insp, DTpred))
## build a random forest model
RFmodel<-randomForest(as.factor(training$Insp)~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
## build a random forest model
RFmodel<-randomForest(as.factor(training$Insp)~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
RFpred<-predict(RFmodel, validation, type="class")
confusionMatrix(table(validation$Insp, RFpred))
## build adaptive boost model
Adamodel<-adaboost(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training, nIter=2)
pred<-predict(Adamodel, validation)
confusionMatrix(table(validation$Insp, pred$class))
## build adaptive boost model
Adamodel<-adaboost(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training, nIter=2)
pred<-predict(Adamodel, validation)
confusionMatrix(table(validation$Insp, pred$class))
## caret Ensemble to compare multiple models
algorithmList<-c("rpart","rf", "adaboost")
models<-caretList(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training, methodList = algorithmList)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/3.PA/CA/data")
pacman::p_load(tidyverse, readxl, lubridate, mice, caret, rpart, randomForest, fastAdaboost, ROCR)
data<-read.csv("reports.csv", stringsAsFactors = FALSE)
dim(data)
summary(data)
table(data$Insp=="unkn")
table(data$Insp)
rate<-1270/15732*100
rate
md.pattern(data)
md.pattern(data[data$Insp!="unkn",])
md.pattern(data[data$Insp=="fraud",])
ggplot(data[data$Insp!="unkn",], aes(Quant, Val, color=Insp))+
geom_point(alpha=0.5, na.rm = TRUE, size=0.5)+
scale_x_continuous(trans= "log") + scale_y_continuous(trans= "log")
data$Missing<-"None Missng"
data[is.na(data$Quant),]$Missing<-"Quant Missing"
data[is.na(data$Val),]$Missing<-"Val Missing"
data[is.na(data$Quant)&is.na(data$Val),]$Missing<-"Both Missing"
table(data$Missing)
data <- data %>% filter(Missing!="Both Missing")
table(data$Missing)
data$unit_price<-data$Val/data$Quant
price_data<-data %>% filter(Insp!="fraud") %>% group_by(Prod) %>%
summarise(med_price=median(unit_price, na.rm = TRUE),iqr=IQR(unit_price, na.rm = TRUE))
table(is.na(price_data$med_price))
price_data<-price_data %>% filter(!is.na(med_price))
data <- data %>% inner_join(price_data, by="Prod")
noQuant<-data$Missing=="Quant Missing"
data[noQuant, "Quant"]=ceiling(data[noQuant, "Val"]/data[noQuant, "med_price"])
noVal<-data$Missing=="Val Missing"
data[noVal, "Val"]=data[noVal, "Quant"]*data[noVal, "med_price"]
data$unit_price<-data$Val/data$Quant
md.pattern(data)
data$Missing<-as.factor(data$Missing)
inspected<-data[data$Insp!="unkn",]
test<-data[data$Insp=="unkn",]
set.seed(1234)
inTrain<-createDataPartition(inspected$Insp, p=0.7, list = FALSE)
training<-inspected[inTrain,]
validation<-inspected[-inTrain,]
## build decision tree model
DTmodel<-rpart(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
data$relative_price<-2*(data$unit_price-data$med_price)/(data$unit_price+data$med_price)
data$Missing<-as.factor(data$Missing)
inspected<-data[data$Insp!="unkn",]
test<-data[data$Insp=="unkn",]
set.seed(1234)
inTrain<-createDataPartition(inspected$Insp, p=0.7, list = FALSE)
training<-inspected[inTrain,]
validation<-inspected[-inTrain,]
## build decision tree model
DTmodel<-rpart(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
DTpred<-predict(DTmodel, validation, type="class")
CM1<-confusionMatrix(table(validation$Insp, DTpred))
## build a random forest model
RFmodel<-randomForest(as.factor(training$Insp)~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
RFpred<-predict(RFmodel, validation, type="class")
CM2<-confusionMatrix(table(validation$Insp, RFpred))
## build adaptive boost model
Adamodel<-adaboost(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training, nIter=2)
pred<-predict(Adamodel, validation)
CM3<-confusionMatrix(table(validation$Insp, pred$class))
## compare the lift chart of three models
lift_result<-data.frame(Class=validation$Insp)
lift_result$DT<-predict(DTmodel, validation, type="prob")[,"fraud"]
lift_result$RF<-predict(RFmodel, validation, type="prob")[,"fraud"]
adapred<-predict(Adamodel, validation)
lift_result$Ada<-adapred$prob[,1]
head(lift_result)
trellis.par.set(caretTheme())
lift_obj<-lift(Class~DT+RF+Ada, data=lift_result)
ggplot(lift_obj, value=90)+scale_x_continuous(breaks = seq(0,100,10))
result<-resamples(CM1,CM2,CM3)
result<-resamples(list(CM1,CM2,CM3))
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/3.PA/CA/data")
pacman::p_load(tidyverse, readxl, lubridate, mice, caret, rpart, randomForest, fastAdaboost, ROCR, mlbench)
result<-resamples(list(CM1,CM2,CM3))
result<-resamples(list(DTmodel,RFmodel,Adamodel))
resamples(list(DTmodel,RFmodel,Adamodel))
DTmodel<-train(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training,method="rpart")
DTmodel<-train(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training,method="rpart")
DTpred<-predict(DTmodel, validation, type="class")
RFmodel<-train(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training, method="rf")
X<-as.matrix(iris[1:150,1:4])
pcaX<-pca(X)
pcaX<-princomp(X)
svdX<-svd(X)
summary(pcaX)
str(pcaX)
pcaX$scale
pcaX$loadings
pcaX$scores
summary(svdX)
str(svdX)
irisPCA<-data.frame(pcaX$scores, iris$Species)
head(irisPCA)
library(ggplot2)
ggplot(irisPCA, aes(x=Comp.1, y=Comp.2, color=iris.Species))+geom_point()
pcaX$scale
svdX$v
pcaX$loadings
diag(d)
diag(svdX$d)
svdX$u*diag(svdX$d)
svdX$u %*% diag(svdX$d)
pcaX$scale
pcaX$loadings
diag(svdX$d) %*% t(diag(svd$d))
diag(svdX$d) %*% t(diag(svdX$d))
diag(svdX$d) %*% t(diag(svdX$v))
diag(svdX$d) %*% t(svdX$v)
pcaX$scores
svdX$v %*% t(svdX$v)
pcaX$scale
pcaX$loadings
c<-t(X) %*% X/149
svdc<-svd(c)
summary(svdc)
str(svdc)
svdc$u
svdc$v
head(pcaX$scores)
head(X %*% t(svdc$u))
head(X)
head(X %*% svdc$u)
head(pcaX$scores)
svdc$d
svdX$d
svdX$d^2/149
test<-data.matrix(seq(3),seq(3))
test
test<-data.matrix(seq(3),seq(3))
test
?data.matrix
seq(3)
test<-data.matrix(seq(3),seq(3)*2)
test
test<-data.matrix(x=seq(3),y=seq(3))
test<-data.frame(x=seq(3),y=seq(3))
test
pca(test)
prcomp(test)
pcatest<-prcomp(test)
pcatest$scale
pcatest$x
getwd()
install.packages("tidytext")
install.packages("shinyjs")
shiny::runApp('Desktop/Shopify/Recommendation')
shiny::runApp('Desktop/Shopify/Recommendation')
runApp('Desktop/Shopify/Recommendation')
########## Create User-Ratings Matrix
# Import reviews.csv
raw_events <- read.csv("reviews.csv", header=TRUE, sep=",")
setwd("~/Desktop/Shopify")
##the app_icon file have been saved, it have the url for icon, title, rating and tagline for app display
app_icons<-read.csv("app_icon.csv", stringsAsFactors = TRUE)
setwd("~/Desktop/Shopify/Recommendation")
##the app_icon file have been saved, it have the url for icon, title, rating and tagline for app display
app_icons<-read.csv("app_icon.csv", stringsAsFactors = TRUE)
View(app_icons)
save(app_icons, "app_icon.Rdata")
save(app_icons, file="app_icon.Rdata")
shiny::runApp()
# please update directory before running the code
# setwd("D:/NUS/ISS-EBAG/EBA5002 - Core Analytics Techniques/CA 2/Shopify Dataset")
#setwd("C:/Users/sophi/Desktop/NUS Master/S2-01 CA2/shopify")
raw_events <- read.csv("reviews.csv", header=TRUE, sep=",") # transaction format!
setwd("~/Desktop/Shopify")
# please update directory before running the code
# setwd("D:/NUS/ISS-EBAG/EBA5002 - Core Analytics Techniques/CA 2/Shopify Dataset")
#setwd("C:/Users/sophi/Desktop/NUS Master/S2-01 CA2/shopify")
raw_events <- read.csv("reviews.csv", header=TRUE, sep=",") # transaction format!
raw_events[1,]
?write.csv
write.csv(raw_events[,-1], "reviews.csv", row.names = FALSE)
# please update directory before running the code
# setwd("D:/NUS/ISS-EBAG/EBA5002 - Core Analytics Techniques/CA 2/Shopify Dataset")
#setwd("C:/Users/sophi/Desktop/NUS Master/S2-01 CA2/shopify")
raw_events <- read.csv("reviews.csv", header=TRUE, sep=",") # transaction format!
raw_events[1,]
names(raw_events)
events = raw_events[c('author','app_id','rating')]
# names(events) = c("author", "app_i", "rating")
length(unique(events$app_id)) # show #books
#3733
length(unique(events$author)) # show #users
# extract only the explicit ratings
events = events[events$rating > 0,]
nb = length(unique(events$app_id)); nb # 3733,
nu = length(unique(events$author)); nu # 299316
# eliminate users with too few ratings
ucnts = aggregate(app_id ~ author, data = events, FUN=length)
head(ucnts)
colnames(ucnts) = c("author","numitems")
activeusers = ucnts$author[ucnts$numitems >= 3 & ucnts$author!=""] ; length(activeusers) ; length(activeusers)/nu#29631
# active users
# > 20, count = 172
# > 10, count = 1338
# > 5,  count = 8547
# > 4, count = 14903
# > 3, count = 29631
# > 2, count = 74582
ev = events[events$author %in% activeusers,]
dim(ev); nrow(ev)/ nrow(raw_events)
head(ev)
# eliminate apps with too few ratings
bcnts = aggregate(author ~ app_id, data = events, FUN=length)
library(tidyverse)
colnames(bcnts) = c("app_id","numusers")
bcnts %>% arrange(numusers) %>% head()
length(unique(bcnts$app_id))
bcnts %>% arrange(-numusers) %>% head()
popularapps = bcnts$app_id[bcnts$numusers >= 1] ;
length(popularapps);
length(popularapps)/nb #3237
# events["author"][events["author"]==""]
head(events)
dim(ev)
rm(ucnts)
rm(bcnts)
rm(events)
?acast
library(tictoc)
library(dplyr)
?acast
users = acast(ev, author ~ app_id, value.var = "rating", fun.aggregate = mean) # we use an aggregation function since some users have rated the same book more than once
library(reshape2)
# eliminate users with too few ratings
ucnts = aggregate(app_id ~ author, data = events, FUN=length)
users = acast(ev, author ~ app_id, value.var = "rating", fun.aggregate = mean) # we use an aggregation function since some users have rated the same book more than once
dim(users)
length(popularapps)/nb #3237
length(popularapps);
# names(events) = c("author", "app_i", "rating")
length(unique(events$app_id)) # show #books
#3733
length(unique(events$author)) # show #users
length(popularapps);
length(popularapps)/nb #3237
ev = ev[ev$app_id %in% popularapps,]
dim(ev)
head(ev)
length(unique(ev$author))
length(unique(ev$app_id))
?aggregate
length(unique(bcnt$app_id))
raw_events <- read.csv("reviews.csv", header=TRUE, sep=",") # transaction format!
raw_events[1,]
names(raw_events)
events = raw_events[c('author','app_id','rating')]
# names(events) = c("author", "app_i", "rating")
length(unique(events$app_id)) # show #books
#3733
length(unique(events$author)) # show #users
#299,316
# extract only the explicit ratings
events = events[events$rating > 0,]
nb = length(unique(events$app_id)); nb # 3733,
nu = length(unique(events$author)); nu # 299316
# eliminate users with too few ratings
ucnts = aggregate(app_id ~ author, data = events, FUN=length)
head(ucnts)
colnames(ucnts) = c("author","numitems")
activeusers = ucnts$author[ucnts$numitems >= 3 & ucnts$author!=""] ; length(activeusers) ; length(activeusers)/nu#29631
# active users
# > 20, count = 172
# > 10, count = 1338
# > 5,  count = 8547
# > 4, count = 14903
# > 3, count = 29631
# > 2, count = 74582
ev = events[events$author %in% activeusers,]
dim(ev); nrow(ev)/ nrow(raw_events)
# users count
# > 10, 19418, 3
# > 5, 63073     3, 0.1410029
# > 4, 88497     3
# > 3, 132681, 3, 29.66152%
# > 2, 222583, 3, 49.7%
head(ev)
bcnts = aggregate(author ~ app_id, data = events, FUN=length)
colnames(bcnts) = c("app_id","numusers")
popularapps = bcnts$app_id[bcnts$numusers >= 1] ;
length(popularapps);
length(popularapps)/nb #3237
# apps count
# >= 15, 1670, 0.4473614
# >= 10, 1957, 0.5242432
# >= 5, 2513, 0.6731851
# >= 3, 3237, 0.867131
# >= 2, 3237, 0.867131
# <= 1, 3733, 1
# events["author"][events["author"]==""]
head(events)
ev = ev[ev$app_id %in% popularapps,]
dim(ev)
length(unique(bcnts$app_id))
sum(bcnts$numusers >= 1)
length(popularapps);
length(popularapps)/nb #3237
# events["author"][events["author"]==""]
head(events)
dim(ev)
ev = ev[ev$app_id %in% popularapps,]
dim(ev)
users = acast(ev, author ~ app_id, value.var = "rating", fun.aggregate = mean) # we use an aggregation function since some users have rated the same book more than once
# users = sweep(users, 1, rowMeans(users, na.rm=TRUE))  # normalise the data
toc()
?read.csv
dim(users) #
# (users, apps)
# colnames(users)
head(users)
# we can also try pre-normalising the data and then using cosine similarity
meanusers = rowMeans(users, na.rm=TRUE)
dim(users)
raw_events <- read.csv("reviews.csv", header=TRUE, sep=",")
events = raw_events[c('author','app_id','rating')]
head(events)
summary(events)
table(events$rating)
dim(events) # 447,317 review records
# Count number of unique apps and reviewers
length(unique(events$app_id)) # 3,733 apps
length(unique(events$author)) # 299,316 reviewers
# Select active reviewers (>= 2 reviews)
ucnts = aggregate(app_id ~ author, data = events, FUN=length)
colnames(ucnts) = c("author","review_count")
activeusers = ucnts$author[ucnts$review_count >= 3]
length(activeusers) # 29,631 active reviewers with >= 3 reviews
active_events = events[events$author %in% activeusers,]
dim(active_events) # 132,681 reviews left (out of initial count of 447,317)
length(unique(active_events$app_id)) # 3,075 apps
head(active_events)
# Create User-Ratings Matrix for active reviewers
active_events <- active_events %>% group_by(author,app_id) %>%
summarize(rating=mean(rating)) # To get mean rating for reviews by same author for same app_id
users1 <- spread(active_events,app_id,rating,fill=0)
object.size(users1)
object.size(users1)/(1024*1024)
dim(users)
rm(users1)
# we can also try pre-normalising the data and then using cosine similarity
meanusers = rowMeans(users, na.rm=TRUE)
normalizedusers = sweep(users, 1, meanusers)   # subtract user (row) means
itemsimsNC = getitemsimsmatrix(normalizedusers , simfun=cosinesim)
source('~/Desktop/Shopify/CF-demolib-v3.R', echo=TRUE)
tic("Rating Matrix Creation Runtime")
itemsimsNC = getitemsimsmatrix(normalizedusers , simfun=cosinesim)
toc()
