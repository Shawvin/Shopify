pred_sg
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/3.PA/CA/data")
pacman::p_load(tidyverse, readxl, lubridate, mice, caret, rpart, randomForest, fastAdaboost, ROCR, caretEnsemble)
data<-read.csv("reports.csv", stringsAsFactors = FALSE)
dim(data)
summary(data)
table(data$Insp=="unkn")
table(data$Insp)
rate<-1270/15732*100
rate
md.pattern(data)
md.pattern(data[data$Insp!="unkn",])
md.pattern(data[data$Insp=="fraud",])
ggplot(data[data$Insp!="unkn",], aes(Quant, Val, color=Insp))+
geom_point(alpha=0.5, na.rm = TRUE, size=0.5)+
scale_x_continuous(trans= "log") + scale_y_continuous(trans= "log")
data$Missing<-"None Missng"
data[is.na(data$Quant),]$Missing<-"Quant Missing"
data[is.na(data$Val),]$Missing<-"Val Missing"
data[is.na(data$Quant)&is.na(data$Val),]$Missing<-"Both Missing"
table(data$Missing)
data <- data %>% filter(Missing!="Both Missing")
table(data$Missing)
data$unit_price<-data$Val/data$Quant
price_data<-data %>% filter(Insp!="fraud") %>% group_by(Prod) %>%
summarise(med_price=median(unit_price, na.rm = TRUE),iqr=IQR(unit_price, na.rm = TRUE))
table(is.na(price_data$med_price))
price_data<-price_data %>% filter(!is.na(med_price))
data <- data %>% inner_join(price_data, by="Prod")
noQuant<-data$Missing=="Quant Missing"
data[noQuant, "Quant"]=ceiling(data[noQuant, "Val"]/data[noQuant, "med_price"])
noVal<-data$Missing=="Val Missing"
data[noVal, "Val"]=data[noVal, "Quant"]*data[noVal, "med_price"]
data$unit_price<-data$Val/data$Quant
md.pattern(data)
data$relative_price<-2*(data$unit_price-data$med_price)/(data$unit_price+data$med_price)
data$Missing<-as.factor(data$Missing)
inspected<-data[data$Insp!="unkn",]
test<-data[data$Insp=="unkn",]
set.seed(1234)
inTrain<-createDataPartition(inspected$Insp, p=0.7, list = FALSE)
training<-inspected[inTrain,]
validation<-inspected[-inTrain,]
## build decision tree model
DTmodel<-rpart(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
## build decision tree model
DTmodel<-rpart(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
DTpred<-predict(DTmodel, validation, type="class")
DTpred<-predict(DTmodel, validation, type="class")
confusionMatrix(table(validation$Insp, DTpred))
## build a random forest model
RFmodel<-randomForest(as.factor(training$Insp)~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
## build a random forest model
RFmodel<-randomForest(as.factor(training$Insp)~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
RFpred<-predict(RFmodel, validation, type="class")
confusionMatrix(table(validation$Insp, RFpred))
## build adaptive boost model
Adamodel<-adaboost(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training, nIter=2)
pred<-predict(Adamodel, validation)
confusionMatrix(table(validation$Insp, pred$class))
## build adaptive boost model
Adamodel<-adaboost(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training, nIter=2)
pred<-predict(Adamodel, validation)
confusionMatrix(table(validation$Insp, pred$class))
## caret Ensemble to compare multiple models
algorithmList<-c("rpart","rf", "adaboost")
models<-caretList(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training, methodList = algorithmList)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/3.PA/CA/data")
pacman::p_load(tidyverse, readxl, lubridate, mice, caret, rpart, randomForest, fastAdaboost, ROCR)
data<-read.csv("reports.csv", stringsAsFactors = FALSE)
dim(data)
summary(data)
table(data$Insp=="unkn")
table(data$Insp)
rate<-1270/15732*100
rate
md.pattern(data)
md.pattern(data[data$Insp!="unkn",])
md.pattern(data[data$Insp=="fraud",])
ggplot(data[data$Insp!="unkn",], aes(Quant, Val, color=Insp))+
geom_point(alpha=0.5, na.rm = TRUE, size=0.5)+
scale_x_continuous(trans= "log") + scale_y_continuous(trans= "log")
data$Missing<-"None Missng"
data[is.na(data$Quant),]$Missing<-"Quant Missing"
data[is.na(data$Val),]$Missing<-"Val Missing"
data[is.na(data$Quant)&is.na(data$Val),]$Missing<-"Both Missing"
table(data$Missing)
data <- data %>% filter(Missing!="Both Missing")
table(data$Missing)
data$unit_price<-data$Val/data$Quant
price_data<-data %>% filter(Insp!="fraud") %>% group_by(Prod) %>%
summarise(med_price=median(unit_price, na.rm = TRUE),iqr=IQR(unit_price, na.rm = TRUE))
table(is.na(price_data$med_price))
price_data<-price_data %>% filter(!is.na(med_price))
data <- data %>% inner_join(price_data, by="Prod")
noQuant<-data$Missing=="Quant Missing"
data[noQuant, "Quant"]=ceiling(data[noQuant, "Val"]/data[noQuant, "med_price"])
noVal<-data$Missing=="Val Missing"
data[noVal, "Val"]=data[noVal, "Quant"]*data[noVal, "med_price"]
data$unit_price<-data$Val/data$Quant
md.pattern(data)
data$Missing<-as.factor(data$Missing)
inspected<-data[data$Insp!="unkn",]
test<-data[data$Insp=="unkn",]
set.seed(1234)
inTrain<-createDataPartition(inspected$Insp, p=0.7, list = FALSE)
training<-inspected[inTrain,]
validation<-inspected[-inTrain,]
## build decision tree model
DTmodel<-rpart(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
data$relative_price<-2*(data$unit_price-data$med_price)/(data$unit_price+data$med_price)
data$Missing<-as.factor(data$Missing)
inspected<-data[data$Insp!="unkn",]
test<-data[data$Insp=="unkn",]
set.seed(1234)
inTrain<-createDataPartition(inspected$Insp, p=0.7, list = FALSE)
training<-inspected[inTrain,]
validation<-inspected[-inTrain,]
## build decision tree model
DTmodel<-rpart(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
DTpred<-predict(DTmodel, validation, type="class")
CM1<-confusionMatrix(table(validation$Insp, DTpred))
## build a random forest model
RFmodel<-randomForest(as.factor(training$Insp)~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training)
RFpred<-predict(RFmodel, validation, type="class")
CM2<-confusionMatrix(table(validation$Insp, RFpred))
## build adaptive boost model
Adamodel<-adaboost(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training, nIter=2)
pred<-predict(Adamodel, validation)
CM3<-confusionMatrix(table(validation$Insp, pred$class))
## compare the lift chart of three models
lift_result<-data.frame(Class=validation$Insp)
lift_result$DT<-predict(DTmodel, validation, type="prob")[,"fraud"]
lift_result$RF<-predict(RFmodel, validation, type="prob")[,"fraud"]
adapred<-predict(Adamodel, validation)
lift_result$Ada<-adapred$prob[,1]
head(lift_result)
trellis.par.set(caretTheme())
lift_obj<-lift(Class~DT+RF+Ada, data=lift_result)
ggplot(lift_obj, value=90)+scale_x_continuous(breaks = seq(0,100,10))
result<-resamples(CM1,CM2,CM3)
result<-resamples(list(CM1,CM2,CM3))
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/3.PA/CA/data")
pacman::p_load(tidyverse, readxl, lubridate, mice, caret, rpart, randomForest, fastAdaboost, ROCR, mlbench)
result<-resamples(list(CM1,CM2,CM3))
result<-resamples(list(DTmodel,RFmodel,Adamodel))
resamples(list(DTmodel,RFmodel,Adamodel))
DTmodel<-train(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training,method="rpart")
DTmodel<-train(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training,method="rpart")
DTpred<-predict(DTmodel, validation, type="class")
RFmodel<-train(Insp~Val+Quant+Missing+unit_price+med_price+iqr+relative_price, data=training, method="rf")
X<-as.matrix(iris[1:150,1:4])
pcaX<-pca(X)
pcaX<-princomp(X)
svdX<-svd(X)
summary(pcaX)
str(pcaX)
pcaX$scale
pcaX$loadings
pcaX$scores
summary(svdX)
str(svdX)
irisPCA<-data.frame(pcaX$scores, iris$Species)
head(irisPCA)
library(ggplot2)
ggplot(irisPCA, aes(x=Comp.1, y=Comp.2, color=iris.Species))+geom_point()
pcaX$scale
svdX$v
pcaX$loadings
diag(d)
diag(svdX$d)
svdX$u*diag(svdX$d)
svdX$u %*% diag(svdX$d)
pcaX$scale
pcaX$loadings
diag(svdX$d) %*% t(diag(svd$d))
diag(svdX$d) %*% t(diag(svdX$d))
diag(svdX$d) %*% t(diag(svdX$v))
diag(svdX$d) %*% t(svdX$v)
pcaX$scores
svdX$v %*% t(svdX$v)
pcaX$scale
pcaX$loadings
c<-t(X) %*% X/149
svdc<-svd(c)
summary(svdc)
str(svdc)
svdc$u
svdc$v
head(pcaX$scores)
head(X %*% t(svdc$u))
head(X)
head(X %*% svdc$u)
head(pcaX$scores)
svdc$d
svdX$d
svdX$d^2/149
test<-data.matrix(seq(3),seq(3))
test
test<-data.matrix(seq(3),seq(3))
test
?data.matrix
seq(3)
test<-data.matrix(seq(3),seq(3)*2)
test
test<-data.matrix(x=seq(3),y=seq(3))
test<-data.frame(x=seq(3),y=seq(3))
test
pca(test)
prcomp(test)
pcatest<-prcomp(test)
pcatest$scale
pcatest$x
getwd()
install.packages("tidytext")
install.packages("shinyjs")
shiny::runApp('Desktop/Shopify/Recommendation')
shiny::runApp('Desktop/Shopify/Recommendation')
library(tm)
library(stringr)
library(SnowballC)
library(slam)
library(dplyr)
library(Matrix)
library(tidyr)
setwd("~/Desktop/Shopify")
raw_events <- read.csv("reviews.csv", header=TRUE, sep=",") # transaction format!
raw_events[1,]
names(raw_events)
events = raw_events[c('author','app_id','rating')]
# Examine events dataframe
head(events) # Author, app_id, rating
summary(events) # ratings between 1-5
# Count the number of unique apps and reviewers
length(unique(events$app_id)) # 3733 apps
length(unique(events$author)) # 299,316 reviewers
# Select only the active reviewers (>= 10 reviews)
ucnts = aggregate(app_id ~ author, data = events, FUN=length)
colnames(ucnts) = c("author","numitems")
activeusers = ucnts$author[ucnts$numitems >= 10]
length(activeusers) # 1338 active reviewers
active_events = events[events$author %in% activeusers,]
length(unique(active_events$app_id)) # 1946 apps
dim(active_events) # Remaining no of reviews = 19418
head(active_events)
# Create User-Ratings Matrix for active reviewers
active_events <- active_events %>% group_by(author,app_id) %>%
summarize(rating=mean(rating))
raw_events <- read.csv("reviews.csv", header=TRUE, sep=",") # transaction format!
raw_events[1,]
names(raw_events)
events = raw_events[c('author','app_id','rating')]
# Examine events dataframe
head(events) # Author, app_id, rating
summary(events) # ratings between 1-5
# Count the number of unique apps and reviewers
length(unique(events$app_id)) # 3733 apps
length(unique(events$author)) # 299,316 reviewers
# Select only the active reviewers (>= 10 reviews)
ucnts = aggregate(app_id ~ author, data = events, FUN=length)
colnames(ucnts) = c("author","numitems")
activeusers = ucnts$author[ucnts$numitems >= 10]
length(activeusers) # 1338 active reviewers
active_events = events[events$author %in% activeusers,]
length(unique(active_events$app_id)) # 1946 apps
dim(active_events) # Remaining no of reviews = 19418
head(active_events)
# Create User-Ratings Matrix for active reviewers
active_events <- active_events %>% group_by(author,app_id) %>%
summarize(rating=mean(rating))
########### To create the User-Ratings Matrix
# Import Reviews.csv
raw_events <- read.csv("reviews.csv", header=TRUE, sep=",") # transaction format!
users <- spread(active_events,app_id,rating,fill=0)
users <- users[-1,]
users_matrix <- as.matrix(users) # user-rating matrix is a large matrix, 20.1Mb
apps <- read.csv("apps.csv", stringsAsFactor=FALSE, encoding="UTF-8")
apps <- apps %>% rename(app_id = id)
head(apps) # 12 variables - description and tagline contain most text data
dim(apps)
# Read key_benefits Data, and join text from title and description by app_id
key_benefits <- read.csv("key_benefits.csv", stringsAsFactor=FALSE, encoding="UTF-8") # 3 variables - appID, title, description
key_benefits$titledescription <- paste(key_benefits$title, key_benefits$description,sep=" ")
key_benefits2 <- aggregate(titledescription ~ app_id, data = key_benefits, paste, collapse = " ")
# Read apps_categories Data
apps_categories <- read.csv("apps_categories.csv", stringsAsFactor=FALSE, encoding="UTF-8") # 2 variables - appID, catID
# Read categories Data
categories <- read.csv("categories.csv", stringsAsFactor=FALSE, encoding="UTF-8") # 2 variables - catID, cat_name
categories <- categories %>% rename(category_id = id, app_category = title)
# Add key_benefits text to apps dataframe
apps <- left_join(apps,key_benefits2, by="app_id")
dim(apps)
apps$text <- paste(apps$description, apps$tagline, apps$titledescription, sep=" ")
# Add category feature to apps dataframe
# apps <- inner_join(apps,apps_categories, by="app_id")
# apps <- inner_join(apps,categories, by="category_id")
# head(apps)
# Keep Title and Description Columns
apps_subset <- apps[, c('app_id','title', 'text')]
head(apps_subset)
apps_subset <- apps_subset[apps_subset$app_id %in% unique(active_events$app_id),]
head(apps_subset)
dim(apps_subset) # again left 1946
# Remove non-ASCII characters with space, replace \n with space
apps_subset$title <- iconv(apps_subset$title, "UTF-8", "ASCII",sub='')
apps_subset$text <- iconv(apps_subset$text, "UTF-8", "ASCII",sub='')
apps_subset$text <- str_replace_all(apps_subset$text, "[\n]" , "")
apps_subset[10,] # Test case to check if replace was successful
# Text Pre-processing
corpus <- VCorpus(VectorSource(apps_subset$text))
for(i in 1:5){
print(corpus[[i]][1])
}
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, stripWhitespace)
# Create Document Term Matrix
dtm <- DocumentTermMatrix(corpus)
dtm_ti <- weightTfIdf(dtm)
dtm_ti
# Convert to Dense Matrix, add app_id column, sort by app_id
dtm_ti_densematrix<-as.matrix(dtm_ti) # 169.4Mb - matrix of all the term frequencies in the DTM by app_id(rows)
rownames(dtm_ti_densematrix) <- apps_subset$app_id
dim(dtm_ti_densematrix)
View(raw_events)
itemmatrix <- dtm_ti_densematrix[order(rownames(dtm_ti_densematrix)),]
dim(itemmatrix)
dim(users_matrix)
rownames(itemmatrix)[1:10]
colnames(users_matrix)[1:11]
users <- spread(active_events,app_id,rating,fill=0)
dim(users)
users <- users[-1,]
users_matrix <- as.matrix(users[,-1]) # user-rating matrix is a large matrix, 20.1Mb
dim(users_matrix)
colnames(users_matrix)[1:11]
rownames(users_matrix)<-users$author
dim(users_matrix)
dim(item_matrix)
dim(itemmatrix)
userprofile <- user_matrix %*% itemmatrix # %*% = matrix multiplication. Need m x n by n x p
userprofile <- users_matrix %*% itemmatrix # %*% = matrix multiplication. Need m x n by n x p
rownames(userprofile)[1:10]
colnames(userprofile)[1:10]
dim(userprofile)
dim(userprofile)
dim(itemmatrix)
# To get similarity matrix (WORK IN PROGRESS) - to find similar users
sim_mat <- userprofile %*% t(itemmatrix)
rownames(sim_mat)[1:10]
colnames(sim_mat)[1:10]
dim(sim_mat)
?sweep
A<-array(1:24, dim=4:2)
A
A<-array(1:24, dim=4)
A
A<-matrix(1:24,nrow=4)
A
sweep(A)
sweep(A, 1, mean)
sweep(A, 1, rowMean)
sweep(A, 1, rowMeans)
?array
sweep(A, 1, rowMeans(A))
rowMeans(A)
dim(A)
cor(A)
cor(t(A))
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
install.packages("Lsa")
library(Lsa)
library(Lsa)
install.packages("Lsa")
install.packages("lsa")
library(lsa)
# To get similarity matrix (WORK IN PROGRESS) - to find similar users
#sim_mat <- userprofile %*% t(itemmatrix)
sim_mat<-cosine(userprofile, itemmatrix)
# To get similarity matrix (WORK IN PROGRESS) - to find similar users
#sim_mat <- userprofile %*% t(itemmatrix)
sim_mat<-cosine(userprofile, t(itemmatrix))
library(lsa)
denominator<-sqrt(rowSums(userprofile^2)) %>% t(sqrt(rowSums(itemmatrix^2)))
denominator<-sqrt(rowSums(userprofile^2)) %*% t(sqrt(rowSums(itemmatrix^2)))
dim(denominator)
dim(sim_mat)
cosine_sim_mat<-sim_mat/denominator
View(cosine_sim_mat)
View(cosine_sim_mat)
View(apps_categories)
View(apps_categories)
View(categories)
View(categories)
runApp('Recommendation')
runApp('Recommendation')
View(users)
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
users$author
runApp('Recommendation')
runApp('Recommendation')
dat<-read.csv("apps.csv", encoding="UTF-8")
View(dat)
dat2<-dat[,c(3,6,7)] %>% unique()
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
?sort
?order
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
dat2$icon[1]
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
View(dat)
View(dat)
View(dat)
dat2<-dat[,c(3,6,7,10)] %>% unique()
dat2<-dat2[order(dat2$rating,decreasing=TRUE),]
dat2$titles<-paste(dat2$title, "\t(",dat2$rating, ")", sep="")
icons<-dat2$icon[1:100]
runApp('Recommendation')
runApp()
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
View(dat)
runApp('Recommendation')
View(dat2)
dat2<-dat[,c(3,6,7,11)] %>% unique()
dat2<-dat2[order(dat2$rating,decreasing=TRUE),]
dat2$titles<-paste(dat2$title, "\t(",dat2$rating, ")", sep="")
icons<-dat2$icon[1:100]
names(icons)<-sapply(1:length(icons), function(x) paste("id",x, sep=""))
titles<-dat2$titles[1:100]
names(titles)<-names(icons)
runApp('Recommendation')
runApp('Recommendation')
dat2[10,"tagline"]
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
runApp('Recommendation')
# Read categories Data
categories <- read.csv("categories.csv", stringsAsFactor=FALSE, encoding="UTF-8") # 2 variables - catID, cat_name
runApp('Recommendation')
runApp()
runApp('Recommendation')
View(itemmatrix)
dim(itemmatrix)
dim(userprofile)
runApp('Recommendation')
setwd("~/Desktop/Shopify/Recommendation")
runApp()
categories <- categories %>% rename(category_id = id, app_category = title)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
colnames(sim_mat)
colnames(sim_mat)[1:10]
c('a','b','c') %in% colnames(sim_mat)[1:10]
colnames(sim_mat)[1:10] %in% colnames(sim_mat)[1:10]
apps$app_id
colnames(sim_mat)[1:10] %in% apps_categories$app_id
str(apps_subset)
str(active_events)
categories[,2]
View(apps_subset)
View(active_events)
categories %>% select(app_category)
gather()
?gather
runApp()
View(users)
runApp()
getwd()
